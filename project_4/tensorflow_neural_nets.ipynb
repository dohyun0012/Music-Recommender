{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define an operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mult = tf.mul(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "* multi-dimensional vectors\n",
    "\n",
    "## Variables\n",
    "* used to represent parameters of the model\n",
    "* variables can be updated\n",
    "* in-memory buffers that contain tensors\n",
    "* survive across multiple executions of a graph (instead of being wiped and rewritten each time like normal tensors)\n",
    "* Requirements for variables\n",
    "    * must be explicitly initialized before a graph is executed\n",
    "    * can be updated with gradient methods\n",
    "    * can save the state to disk and load them for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight variables in NNs\n",
    "```python\n",
    "weights = tf.Variable(tf.truncated_normal([300,200],stddev=0.5,name=\"weights\", mean=0.0))\n",
    "```\n",
    "* the above code could be matrix tying together 2 layers of a neural network, the 1st layer would have 300 neurons and the 2nd would have 200 neurons\n",
    "* optional argument trainable=False if you don't want the weights to be able to update (maybe for some kind of static word vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly initialize weights that could tie a neural network layer together\n",
    "## this would have input shape 300 dim and output shape 200 dim\n",
    "weights = tf.Variable(tf.truncated_normal([300,200],stddev=0.5,name=\"weights\", mean=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when tf.variable is called\n",
    "* 3 operations added to computation graph\n",
    "    1. operation producing tensor the tensor we use to initialize the variable\n",
    "    2. tf.assign operation --> responsible for filling the variable with intializing tensor prior to the variable's use\n",
    "    3. variable operation --> holds the current value of the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have to initialize tensorflow variables before running the graph\n",
    "```python\n",
    "tf.initialize_all_variables()\n",
    "```\n",
    "This triggers all of the tf.assign operations in the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Operations\n",
    "* abstract transformations that are applied to tensors in the computation graph\n",
    "* operations can be given a name for easy reference into the computation graph\n",
    "* operations consist of one or more kernels\n",
    "    * kernel is a device-specific implementation (GPU vs CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder Tensors\n",
    "### Control how input is passed into deep models\n",
    "Can't use standard variables because they are only meant to be initialized once\n",
    "\n",
    "placeholder tensors are populated every single time the computation graph is run\n",
    "\n",
    "```python\n",
    "x = tf.placeholder(tf.float32, name=\"x\", shape=[None,784])\n",
    "W = tf.Variable(tf.random_uniform([784,10],-1,1),name=\"W\")\n",
    "multiply = tf.matmul(x,W)\n",
    "```\n",
    "\n",
    "### Code above:\n",
    "* x:\n",
    "    * represents mini-batch of data \n",
    "    * 784 columns (each sample has 784 dimensions)\n",
    "    * undefined rows --> means x can be initialized with any number of data samples (any size mini-batch)\n",
    "* W:\n",
    "    * can represent the weight matrix tying a layer of a neural network to another layer (be that layer another hidden layer or an output/softmax layer\n",
    "    * input dimension of 784 \n",
    "    * output dimension of 10 (can represent # of classes)\n",
    "    * -1,1 represents the range that the numbers are drawn from\n",
    "* placeholders need to be filled every time the computation graph (or subgraph) is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sessions\n",
    "* how tensorflow program interacts with computation graph\n",
    "* builds the graph, initializes variables, and runs the graph\n",
    "### example below, but ignore the read data thing so i dont have the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-579ba858d8ce>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-579ba858d8ce>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from read_data import get_minibatch()\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# from read_data import get_minibatch()\n",
    "\n",
    "\n",
    "# Describing the computational graph\n",
    "x = tf.placeholder(tf.float32, name=\"x\", shape=[None,784])\n",
    "W = tf.Variable(tf.truncated_normal([784,10],-1,1),name=\"W\")\n",
    "b = tf.Variable(tf.zeros([10]), name=\"biases\")\n",
    "\n",
    "output = tf.matmul(x,W) + b\n",
    "\n",
    "# initialize the variables\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# define the tensorflow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# performs the initialization\n",
    "sess.run(init_op)\n",
    "\n",
    "# define feed_dict, which fills the placeholders with necessary input data\n",
    "## define more dictionary entries if the graph has multiple places where data will go\n",
    "feed_dict = {\"x\": get_minibatch()}\n",
    "\n",
    "# run the graph again but pass in the values you want inside the placeholders\n",
    "sess.run(output, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sess.run() used to:\n",
    "* initialize variables\n",
    "* put data into placeholders (input slots)\n",
    "* train the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigating Variable Scopes and Sharing Variables\n",
    "* complex models require a lot of re-using and sharing large sets of variables --> will want to instantiate in one place\n",
    "\n",
    "# Code below should be used in a case where you want to be able to create a DIFFERENT network everytime you call it\n",
    "* NOTE if you want to reuse or access the same network you need to define it different (will be explained below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Defines a network with 6 variables and 3 layers'''\n",
    "def my_network(input):\n",
    "    # first layer\n",
    "     ## takes input of dim 784, outputs tensor with dim=100\n",
    "    W_1 = tf.Variable(tf.random_uniform([784,100],-1,1),name=\"W_1\")\n",
    "    b_1 = tf.Variable(tf.zeros([100]), name=\"biases_1\")\n",
    "    output_1 = tf.matmul(input,W_1) + b_1\n",
    "    \n",
    "    # second layer\n",
    "     ## takes input of dim 100, outputs tensor with dim=50\n",
    "    W_2 = tf.Variable(tf.random_uniform([100,50],-1,1),name=\"W_2\")\n",
    "    b_2 = tf.Variable(tf.zeros([50]),name=\"biases_2\")\n",
    "    output_2 = tf.matmul(output_1,W_2) + b_2 # output_1 is the input to this layer\n",
    "    \n",
    "    # third layer\n",
    "     ## takes input of dim 50, outputs tensor with dim=10 (should be the # of classes)\n",
    "    W_3 = tf.Variable(tf.random_uniform([50,10],-1,1),name=\"W_3\")\n",
    "    b_3 = tf.Variable(tf.zeros([10]),name=\"biases_3\")\n",
    "    output_3 = tf.matmul(output_2,W_3) + b_3\n",
    "    \n",
    "    # printing names (to show that this creates a different network each time)\n",
    "    print \"Printing names of weight parameters\"\n",
    "    print W_1.name, W_2.name, W_3.name\n",
    "    print \"Printing names of bias parameters\"\n",
    "    print b_1.name, b_2.name, b_3.name\n",
    "    \n",
    "    return output_3\n",
    "    \n",
    "    \n",
    "## pass something into this function like:\n",
    "i_1 = tf.placeholder(tf.float32, [1000,784],name=\"i_1\")\n",
    "my_network(i_1)\n",
    "\n",
    "# second time you call this the name of \"W_1\" will actually be \"W_1_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow's variable scoping mechanisms largely controlled by 2 functions\n",
    "1. ```tf.get_variable(<name>,<shape>,<initializer>)``` Checks if a variable with this name exists, retrieves the variable if it does, creates it using the shape and initializer if it doesnt\n",
    "2. ```tf.variable_scope(<scope_name>)```: manages the namespace and determines the scope in which ```tf.get_variable``` operates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(input, weight_shape, bias_shape):\n",
    "    weight_init = tf.random_uniform_initializer(minval=-1,maxval=1)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    \n",
    "    W = tf.get_variable(\"W\",weight_shape, initializer=weight_init)\n",
    "    b = tf.get_variable(\"b\",bias_shape, initializer=bias_init)\n",
    "    \n",
    "    return tf.matmul(input,W) + b\n",
    "\n",
    "def my_network(input):\n",
    "    # define scope of layer_1\n",
    "    with tf.variable_scope(\"layer_1\"):\n",
    "        output_1 = layer(input, [784, 100], [100]) # get the output of the first layer\n",
    "    with tf.variable_scope(\"layer_2\"):\n",
    "        output_2 = layer(output_1, [100,50], [50]) # get the output of the 2nd layer\n",
    "    with tf.variable_scope(\"layer_3\"):\n",
    "        output_3 = layer(output_2, [50,10], [10]) # get the output of the 3rd layer\n",
    "        \n",
    "    return output_3 # this is the final output of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By default sharing the variables is not allowed, but you can turn that off by saying explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models in a \"tower\" is the way to use multiple gpus or cpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model in TensorFlow\n",
    "* p(y=i|x) = softmax(Wx+b)\n",
    "    * goal is to learn values of W and b that most effectively classify the inputs as accurately as possible\n",
    "\n",
    "## Build the model in 4 phases:\n",
    "1. Inference\n",
    "    * produces probability distribution over the output classes given a minibatch\n",
    "2. Loss\n",
    "    * computes the value of the error function (cross-entropy loss for this example)\n",
    "3. training\n",
    "    * computes gradients of model's parameters and updates the model\n",
    "4. evaluate\n",
    "    * determines the effectiveness of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Inference\n",
    "* given minibatch w/ 784-dimensional vectors representing MNIST images\n",
    "* represent log-reg by softmax(Wx+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", [784,10], initializer=init)\n",
    "    b = tf.get_variable(\"b\",[10], initializer=init)\n",
    "    \n",
    "    output = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Loss\n",
    "* average error per data sample\n",
    "* comput cross entropy loss over a minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(output, y):\n",
    "    dot_product = y * tf.log(output)\n",
    "    \n",
    "    # Reduction along axis 0 collapses each column into a single value\n",
    "    # Reduction along axis 1 collapses each row into a single value\n",
    "    # Generally, reduction along axis i collapses the ith dimension of a tensor to size 1\n",
    "    \n",
    "    cross_entropy = -tf.reduce_sum(dot_product, reduction_indices=1)\n",
    "    \n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Training\n",
    "* given cost incurred, compute gradients and modify parameters of model\n",
    "* tensorflow gives access to built-in optimizers that use special train operations when run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Evaluate\n",
    "* put everything into a single computational subgraph to evaluate model on valid or test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    correct_prediction = tf.equal(tf.argmax(output,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full logistic regression in tensor flow with loggin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1000\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # mnist data image of shape 28*28 = 784\n",
    "    x = tf.placeholder(\"float\", [None, 784])\n",
    "    \n",
    "    # 0-9 digits recognition => 10 classes\n",
    "    y = tf.placeholder(\"float\",[None, 10])\n",
    "    \n",
    "    output = inference(x)\n",
    "    \n",
    "    cost = loss(output, y)\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "    train_op = training(cost, global_step)\n",
    "    \n",
    "    eval_op = evaluate(output, y)\n",
    "    \n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    \n",
    "    summary_writer = tf.train.SummaryWriter(\"logistic_logs/\", graph_def=sess.graph_def)\n",
    "    \n",
    "    init_op = tf.initialize_all_variables()\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            minibatch_x, minbatch_y = mnist.train.next_batch(batch_size)\n",
    "            # fit training using batch data\n",
    "            feed_dict = {x: minibatch_x, y: minbatch_y}\n",
    "            sess.run(train_op, feed_dict=feed_dict)\n",
    "            avg_cost += minibatch_cost/total_batch\n",
    "            # display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                val_feed_dict = {\n",
    "                    x: mnist.validation.images,\n",
    "                    y: mnist.validation.labels\n",
    "                }\n",
    "                \n",
    "                accuracy = sess.run(eval_op, feed_dict=val_feed_dict)\n",
    "                print \"Validation error: \", (1-accuracy)\n",
    "                \n",
    "                summary_str = sess.run(summary_op, feed_dict = feed_dict)\n",
    "                summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "                \n",
    "                saver.save(sess, \"logistic_logs/model-checkpoints\")\n",
    "                \n",
    "                print \"Optimization Finished!\"\n",
    " \n",
    "    test_feed_dict = {\n",
    "         x : mnist.test.images,\n",
    "         y : mnist.test.labels\n",
    "    }\n",
    "\n",
    "    accuracy = sess.run(eval_op, feed_dict=test_feed_dict)\n",
    "\n",
    "    print \"Test Accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
